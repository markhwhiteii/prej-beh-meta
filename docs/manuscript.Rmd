---
title: "Historical Trends in Predicting Interracial Behaviors: A Meta-Analysis"
shorttitle: "Racial Attitudes and Behaviors"
author: 
  - name: Mark H. White II
    affiliation: 1
    corresponding: yes
    email: markhwhiteii@gmail.com
affiliation:
  - id: 1
    institution: University of Kansas

author_note: |  
  Thank you to Dr. Elizabeth Tipton and Dr. Wolfgang Viechtbauer for their advice in conducting and communicating the results of the meta-analysis, as well as for their essential `R` packages, `robumeta` and `metafor`, respectively. The Appendix can be found at https://github.com/markhwhiteii/prej-beh-meta.
  
abstract: |
  Researchers often note how self-reports of prejudice have decreased over time, yet evidence of discrimination still persists. Have these explicit measures of interracial attitudes gotten worse at predicting behaviors over time? I performed a meta-analysis to investigate if there is a negative relationship between the year in which a study was conducted and the observed interracial attitude-behavior correspondence. There was a weak relationship between the two before considering covariates; however, there was no relationship after controlling for confounding variables. This lack of relationship was largely attributable to increasing subtlety of behavioral measures in psychological studies over time. Meta-regression analyses found that explicit measures predict obvious and controllable behaviors better than subtle behaviors. The current analysis improved upon statistical flaws in past meta-analyses on the topic, contradicting some previous findings. Most notably, affective measures were not stronger predictors of behavior than cognitive measures—they both had the same predictive power. Relative to other attitude-behavior correspondence meta-analyses, a small overall meta-analytic correlation between interracial attitudes and behavior was found, $r = .21$. I argue that psychologists should use more predictors of behavior (e.g., aspects of the person and the environment) as well as more flexible statistical learning techniques in the focus on predicting prejudiced behavior.

class: man
lang: english
figsintext: yes
figurelist: no
tablelist: no
lineno: no
bibliography:
  - r-references.bib

nocite: |
  @aberson2008friendships, @alreshoud1997arab, @bastide1957stereotypes, @berg1966ethnic, 
  @bernard2014blacks, @biernat2009race, @binder2009does, @boag2010attachment, 
  @boag2012self, @brannon1973attitude, @brazy2007influence, @brief2000just, 
  @butler1978american, @butz2014negative, @christ2010direct, @defleur1958verbal, 
  @defriese1969verbal, @dijker1987emotional, @dovidio1997nature, @dovidio2002implicit, 
  @echebarria2007new, @ewens1972reference, @fendrich1967study, @fendrich1967perceived,
  @gallego2014big, @goff2008space, @gonsalkorale2009bias, @green1972attitudinal,
  @greitemeyer2014employing, @hagiwara2013racial, @heider2005implicit,
  @heider2007improving, @himelstein1963racial, @hofmann2008implicit,
  @holmes2013examination, @howitt1978attitudes, @hraba1996comparison, @jacoby2016lesson,
  @jacoby2015bias, @kamenetzky1956relative, @lee2009islamophobia, @lightbourn2000diasporas,
  @macinnis2012rubber, @malof1962ethnocentrism, @mcconnell2001relations,
  @meeusen2014parent, @montgomery1973predicting, @pettigrew1997generalized, 
  @philip2010understanding, @phillips2012role, @pittinsky2011liking, @plant2003antecedents,
  @richeson2003prejudice, @saucier2003persuasiveness, @schnake1998subtle, 
  @sechrist2000influence, @sechrist2007influence, @sekaquaptewa2003stereotypic,
  @silverman1971relationship, @smith1968verbal, @stephan1989antecedents, @sutton2014social,
  @turner2012behavioural, @turner2014role, @vescio2006effects, @wagner2003ethnic, 
  @warner1969attitude, @webb2011advice, @weitz1972attitude, @katz1975some, @neel2012racial,
  @thomsen2012does
  
output: 
  papaja::apa6_pdf:
    fig_caption: yes
---

```{r message = FALSE, warning = FALSE, include = FALSE}
# packages
library(papaja)
library(readr)
library(dplyr)
library(robumeta)
library(clubSandwich)
library(psych)
library(ggplot2)
library(tidyr)
library(metafor)
library(knitr)

# loading and formatting data
meta <- read_csv("~/Dropbox/CompsMeta/Data/metafinal.csv") %>% 
  mutate(effectid=as.factor(effectid), studyid=as.factor(studyid), 
         attitude=as.factor(attitude), paradigm=as.factor(paradigm))

# data without other and unsure attitudes
meta_noatt3 <- meta %>% 
  filter(attitude != 3) %>% 
  droplevels.data.frame()
```

One of the most common introductions in articles on contemporary forms of prejudice is to note the paradox that self-reports of prejudice have decreased in the recent decades, yet inequality and discrimination continue [e.g., @crosby1980recent; @dovidio2000aversive; @dovidio2004aversive; @frey1986helping; @henry2002symbolic; @pearson2009nature]. For example: "Over the past 40 years, public opinion polls have revealed substantial declines in Whites' endorsement of prejudiced views toward minority groups, and Blacks in particular, in the United States... Despite these declines in overt prejudice, however, evidence of substantial racial disparities and discrimination remain... " [@pearson2009nature, p. 315]. Many social psychological theories have tried to explain this contradiction with what @crandall2003justification call "two-factor theories" of prejudice, including modern [@mcconahay1981has], ambivalent [@katz1988racial], and aversive racism [@dovidio2004aversive]. These theories all propose that (a) people continue to feel and want to express and act on their prejudices, but (b) are motivated to keep themselves from doing so for either internal (e.g., personal standards) or external (e.g., social pressure) reasons [@crandall2003justification]. Thus, decreases in self-reported prejudice over time are not necessarily decreases in genuine attitudes, so discriminatory behaviors continue.  

Researchers across disciplines and settings have examined this contradiction of contemporary racial attitudes, but psychological research provides a unique way to track interracial attitudes and behaviors over time. Academic journals and institutions have kept roughly the same structure for publishing research in the last few decades, providing a consistent record of these relationships over time. Despite the limitations of academic research—such as non-representative samples or contrived laboratory situations—it is a rather valuable source of data, as many researchers have measured interracial attitudes and behaviors within the *same person*, something that data from other institutions and fields cannot speak to.  

If the endorsement of negative interracial attitudes is decreasing at a faster pace than are the corresponding behaviors, an individual's explicit attitudes should be *less* predictive of *that same person's* interracial behaviors today than they were decades ago. Using psychological research from the past 70 years, I will take a meta-analytic approach to test this hypothesis that self-reported racial attitudes have worsened in predictive validity of interracial behaviors over the history of the field.  

## Trends in Interracial Attitudes
It is taken for granted, even among scholars, that the tenor of interracial attitudes has grown more positive over time. But is this really the case? The American National Election Studies [ANES; see @anesguide] and General Social Survey [GSS; see @bobo2012real] provide nationally representative polling data from the last 60 years that illustrate the complexity of racial attitude trends. Given the history of race relations in America, these data focus on the attitudes of White Americans toward Black Americans.  

Support for discriminatory policies, negative responses to social distance and emotional measures, and endorsement of negative stereotypes have all decreased since the 1940s. Favoring segregation in general (polling at 5% in 1978) and keeping Black people out of White neighborhoods (9% in 1976) were items eventually dropped by the ANES; the GSS dropped an item in the 1980s about keeping Black children in separate schools because so few people endorsed it, as well. Social distance items (e.g., having dinner with a Black person) have shown promising improvements. For example, the GSS data show a decrease in opposition to racial intermarriage from 1990 (about 65%) to 2008 (about 25%), and there was an increase in how many Whites reported feeling equally close to Whites and Blacks from 1996 (about 40%) to 2008 (about 50%). Racial stereotypes were not even included at the beginning of the GSS due to a rapid drop in endorsement of anti-Black stereotypes from the 1940s to the 1970s. However, questions asking whether or not Whites are more intelligent and hardworking than Blacks have been included since 1990; there has been a drop from about 60% to 35% in endorsement of this belief from 1990 to 2008. White Americans are less likely today than decades ago to openly endorse many of these negative attitudes traditionally referred to as "prejudice."  

Not all racial attitudes have improved, however, particularly those about actively promoting Black prosperity—as opposed to just rejecting discriminatory policies—through government programs. According to the ANES, believing the government should (a) *not* help minorities improve their socioeconomic position in society and (b) stay out of ensuring fair treatment for minorities in the job market have *increased* somewhat since the 1960s. The GSS data also show unchanging trends for similar items: The government should *not* give Blacks special treatment to help make up for past discrimination (50-60% range from 1973 to 2007), opposing preferential hiring and promotion of Blacks due to past discrimination (about 90% from 1994 to 2008), and believing that Blacks should work their way up in society without special favors from the government (about 80% from 1994 to 2008). @bobo2012real also note from the GSS data that an increased number of people over time have been responding to race-related questions with "unsure" or "don't know," which is consistent with people suppressing prejudice more over time.  

White Americans clearly do not hold the same explicit negative attitudes about racial minorities today as 60 years ago; however, they do not seem to be moving toward supporting concrete policies that would actively address the legacy of racism and discrimination [@mcconahay1981has]. Despite this critical exception, the majority of polling research shows a decline over time in most forms of explicit negative racial measures used by psychologists. Why have these changes occurred?  

## Norms and Interracial Attitudes
Psychologists and sociologists agree that the decrease in explicitly expressed racial attitudes is due to the change in normative acceptability of openly expressing prejudice since World War II and the Civil Rights Movement [@quillian2006new]. Social psychologists have argued for more than 70 years that social norms are one of the strongest causes of expressing—or not expressing—prejudice [@crandall2005conformity]. @crandall2002social demonstrated that the appropriateness and expression of prejudice are essentially the same construct. Crandall and colleagues asked one group of college students to indicate how much they disliked 105 different targets of prejudice (e.g., Black people, alcoholics, lawyers, police officers) and a separate group of students to indicate how "OK" it is to "feel negative feelings" about each of the groups. Expression and acceptability of prejudice were nearly perfectly correlated, $r = .96$; @cary2014prevalence replicated this finding, observing the exact same point estimate. A number of other studies have corroborated this experimentally in the lab and field; making prejudice normative increases its expression, while making it counter-normative decreases expression [@blanchard1994condemning; @ford2008more; @monteith1996effect; @paluck2011peer; @sechrist2001perceived; @stangor2001changing; @zitek2007role].  

But norms are not the panacea for all of prejudice's ills, and the slope of increasingly positive interracial attitudes is not as linear—or as steep—as we would like it to be. Each step toward racial equality in the United States has provoked a White backlash, from the Black Codes that followed the 13th Amendment to the voter suppression efforts targeted at people of color after Barack Obama's election [@anderson2016white]. Normative pressure can also backfire, causing people to express or re-articulate their prejudices [@lalonde2000political; @legault2011ironic; @omi1994racial; @plant2001responses; @white2017freedom].  

Nevertheless, changing norms about interracial attitudes have limited the expression of traditional forms of prejudice. As these norms have grown stricter over time, I am using the year in which a study was published as a proxy for the normative climate. The primary hypothesis of this paper can thus be rephrased: As norms have gotten more strict against expressing prejudice (i.e., as time has passed), the predictive validity (i.e., correlation between attitude and behavior) of interracial attitude measures has decreased.  

## Trends in Interracial Behaviors
There is evidence for the first part of the paradox of contemporary prejudice: Interracial attitudes are improving (however limited) over time. Is there evidence for the second part—that prejudiced behaviors have *not* improved as much over that same time period? Researchers cannot tell if people really "mean it" when respondents express non-prejudiced attitudes, as it is impossible to measure prejudice *directly*—in a way that is immune to social desirability, unconscious cultural knowledge, and so on [@crandall2003justification]. When contemporary studies of prejudice do not find much of a link between attitudes and behaviors, researchers infer that the changes in interracial attitudes do not necessarily represent changes in one's "true" attitudes. Authors often use this contradiction of contemporary racial attitudes to reel readers in, but are these introductions more catchy than they are veridical? @bobo2012real specifically and unequivocally address the issue: "...the all-to-common sociological assertion that the attitudinal record paints a purely and unduly optimistic picture of race relations at odds with actual behavioral data on segregation, inequality, and discrimination is simply wrong" (p. 73). What does the evidence say?  

The Federal Bureau of Investigation (FBI) yearly hate crime reports show that the number of race- and ethnicity-related hate crimes have decreased since 1996, the first year that official data were collected [@middlebrook2017fascinating]. However, the Southern Poverty Law Center (SPLC) has counted an increase in the number of hate group chapters since 1999 [@potok2016year]. These sources each have their own biases: Law enforcement agencies voluntarily report hate crimes to the FBI, while the SPLC relies on news, media, and voluntary reports from individuals about both hate crimes and non-criminal incidents [@majumder2017higher]. It is unclear if extreme discriminatory behaviors are decreasing, increasing, or being concentrated at fringes of society (i.e., hate groups).  

Considering less extreme behaviors, increases in endorsement of interracial marriage have corresponded with actual rates of interracial marriage increasing [@pew2012rise]; similarly, as self-reported support for residential segregation has dropped, field experiments on racial discrimination in housing markets and real-estate agent behavior have found less discrimination in 2000 than in 1989 [@quillian2006new]. @toosi2012dyadic found in a meta-analysis of dyadic interracial interactions that people interacting with someone of their own race expressed more positive attitudes, more friendly nonverbal behavior, feeling less negative affect, and scored higher on performance tasks than those interacting with someone of a different race. They found that these effect sizes have been shrinking over time, although the trend was only significant with attitudes toward the partner and nonverbal behavior. On the other hand, @saucier2005differences conducted a meta-analysis on interracial helping behavior experiments; looking at 48 effect sizes from 31 articles published since 1967, the authors found *no* relationship between publication year and discrimination in helping behavior.  

Institutional-level disparities have also been thoroughly researched. These inequalities, while not usually considered at the individual level like psychological studies, are still relevant; while the causes of these disparities can exist absent negative behaviors of individuals [@bloome2014racial; @traub2016racial], there are nonetheless individuals acting on behalf of institutions making decisions that negatively harm racial outgroups (e.g., teachers, bankers, doctors). For example, the United States Department of Justice found that Wells Fargo bankers led Black and Hispanic borrowers—due to their race—into sub-prime mortgages and paying higher fees in the mid- to late-2000s [@us2012justice]. Racial inequalities in high school completion, life expectancy, and voter turnout have decreased over time; however, disparities in unemployment, poverty, household income, household wealth, home ownership, and incarceration have remained largely consistent for decades [@economic2013unfinished; @pew2016views; @urban2015wealth; @us2012income]. The evidence reviewed here is quite mixed; the objective of the current meta-analysis is to add to this body of research by considering the relationship between attitudes and behaviors within individuals.  

## How Has Studying Racial Attitudes Changed?
The ways that researchers study prejudice are, of course, subject to the same changing normative environment regarding prejudice expression as participants and respondents in studies and polls. Decisions about what prejudices to study and how to study them reflect the milieu in which the researcher lives; researchers often study what they believe to be wrong, but commonly felt among people [@crandall2013we]. As certain ways of expressing prejudice diminish and other ways emerge, researchers have changed how they study and measure prejudice; @biernat1999racial note a large number of scales being published during the 1990s in response to a changing normative climate about racial attitudes and the subsequent social desirability bias of respondents.  

Consider @bray1950prediction, one of the earliest studies in the present meta-analysis. He uses a scale developed by @likert1932technique that contains an item which asks the participant when they think lynching is justifiable, and another item contains the n-word. Many researchers would not consider putting this language in front of participants, and some participants would justifiably react strongly to the language in the items. This language is far different from the items we generally ask participants now, which focus on attitudes toward affirmative action policies and attributions for why Blacks might not be as well-off as Whites. This change of measurement tools is valuable: Researchers are responding to the social climate in real-time, publishing scales that capture how racial attitudes are expressed contemporarily, ensuring that prejudice can still be accurately measured. This poses a problem for the present meta-analysis, however, as the hypothesized negative relationship between publication year and effect size could be absent simply because researchers are skilled in keeping up with the trends of prejudice expression. I return to this point in the Discussion.  

Perhaps the greatest change in the study of racial attitudes has been the intense focus on measuring automatic [@devine1989stereotypes], implicit [@greenwald1995implicit] racial attitudes, particularly since the creation of the Implicit Association Test [@greenwald1998measuring]. There has been such a concentration on implicit measures that, in under two decades, there are already various meta-analysis investigating these measurement tools [e.g., @greenwald2009understanding; @hofmann2005meta]. In just 14 years, @oswald2013predicting located over 300 correlations involving race-related IAT scores across 86 samples in 46 reports. Meta-analyses and citation counts do not capture the full movement towards focusing on implicit attitudes over explicit attitudes. "Unconscious biases" have been widely discussed in popular culture—and not just the usual outlets for this type of research (e.g., *The New York Times*, *The Atlantic*, *Vox*). Companies like Google and Microsoft are implementing programs to combat "unconscious bias," leading the research to be discussed in *Forbes* [@huet2015rise], *Business Insider* [@feloni2016presentation], *Fortune* [@olson2015corporate], and *The Wall Street Journal* [@van2017seven]. Even far-right media outlets have joined in on the discussion, generally lambasting the concept altogether [e.g., @french2017implicit; @hayward2016implicit].  

Despite this focus on implicit measures, I will only focus on explicit attitudes in the present meta-analysis. The primary research question—has attitude-behavior correspondence in the interracial domain decreased over time?—is based on the idea that, due to the changing normative climate, people are suppressing their overt expressions quicker than they are changing their genuine attitudes and corresponding behaviors [@crandall2003justification]. Implicit attitudes are not as easily controlled and thus not subject to the theoretical phenomenon of interest. Special attention must be paid to this research, nonetheless: Implicit social cognition studies tend to focus on subtler behaviors, while also including measures of explicit attitudes. Various theories predict that implicit attitudes correspond with behaviors that are hard to control (e.g., eye contact in an interracial interaction), while explicit attitudes correspond with behaviors that are controllable [e.g., aggression against someone of another race; for a review, see @friese2009implicit]. Some studies, therefore, are published to demonstrate that explicit attitudes specifically do *not* predict certain behaviors. Given that these studies are newer, this means more recent studies might have weaker effects simply because the behaviors are more subtle (e.g., time spent smiling during interracial interaction) and have more measurement error (i.e., subtle behaviors are harder to measure); a negative relationship between effect size and time then might be simply due to the changes in research design. I address by coding for subtlety of behavior and type of attitude measurement.  

## Previous Reviews and Meta-Analyses
Attitude-behavior correspondence (ABC) has long been an interest to social psychologists. There are a number of previous meta-analyses that focus on ABC across fields of study [e.g., intergroup relations, political science, consumer psychology, environmental studies; @armitage2001efficacy; @cooke2004moderation; @glasman2006forming; @kim1993attitude; @kraus1995attitudes; @notani1998moderators; @sheppard1988theory; @wallace2005which]. Using various different inclusion criteria and statistical analyses, these reviews have contained meta-analytic correlation coefficients ranging from $r = .21$ [@notani1998moderators] to $r = .52$ [@glasman2006forming]. Given this heterogeneity, most meta-analyses focus on moderators of ABC; some do not even report the overall meta-analytic correlation [e.g., @armitage2001efficacy; @cooke2004moderation; @sheppard1988theory]. Consistent across meta-analyses are findings that ABC is higher when: attitudes are easily retrieved from memory, conscious thought goes into attitudes, attitudes are stable over time, attitudes rely on information relevant to the behavior, attitudinal ambivalence is low and certainty is high, and attitudes are experimentally generated in the laboratory—that is, are *not* preexisting attitudes [e.g., @cooke2004moderation; @glasman2006forming].  

Authors generally use two theories to frame these relationships: the theory of planned behavior [@ajzen1991theory] and the MODE model [@fazio1990multiple], which both argue that one's willingness and ability to act on their attitudes increase ABC. The MODE adds that if people are unmotivated or unable to cognitively process relevant information about their attitudes and the behavioral situation, then strongly-held, readily-accessible attitudes will automatically guide behavior. Particularly relevant to this meta-analysis is Ajzen's contention that perceived social norms affect intention to behave, which in turn affects actual behavior; the more normative one perceives an attitude to be, the higher their motivation is to perform the behavior, which in turn positively predicts acting in accordance with those attitudes [@armitage2001efficacy]. My hypothesis that the correlation between interracial attitudes and behaviors should diminish over time is in line with this aspect of the theory of planned behavior, as the publication year is a proxy for subjective norms.  

I am predicting that ABC in the *interracial* domain will shrink as time passes. A useful comparison might be historical ABC trends for attitudes in *general*: @kraus1995attitudes found that ABC was positively correlated with publication year $r = .21$, but was non-significant after controlling for behavior type, $r = .11$; @glasman2006forming, in a meta-analysis of only experimentally-formed attitudes, found that articles published since 1996 had a significantly larger ABC ($r = .55$) than those before that year ($r = .40$).  

ABC has received meta-analytic attention within intergroup relations specifically, as well: Two meta-analyses on intergroup relations generally [@schutz1996strong; @talaska2008legitimating], while others have a particular focus on implicit intergroup attitudes, which are outside the scope of this paper [e.g., @oswald2013predicting]. @schutz1996strong found an overall correlation between attitudes and behavior of $r = .29$, but noted that the sizes varied across setting, behavioral measurement type, sample, etc. Their analysis is descriptive in nature; they do not make a firm theoretical position other than stating effect sizes vary and seem to be larger in situations when participants are more in control of their behavior. They urge researchers to not focus on *if* intergroup attitudes predict intergroup behaviors—it is clear they do—but *when* attitudes and behavior are consistent versus inconsistent. @talaska2008legitimating take up this task by focusing primarily on moderators of interracial attitude and behavior correspondence; their main argument is that ABC in the interracial domain is higher when attitude measures focus on emotions, not beliefs. They also find that year somewhat negatively predicted effect size in their model, $\beta = -.17, p = .07$. Talaska and colleagues, however, treated this finding as a methodological covariate and do not discuss the possible theoretical implications of the finding. Given @talaska2008legitimating, why is the present meta-analysis necessary to test the relationship?  

First, I constructed a data set that was designed to test the year hypothesis specifically. @talaska2008legitimating include implicit and indirect measures of interracial attitudes; these measures are not as easily-controlled and thus not subject to the normative pressure that is the key phenomenon at play behind the year effect. I do not include effects based on these measures. Second, Talaska and colleagues selected peer-reviewed studies published through 2002; I extended this by including dissertations and theses, as well as including studies published through the end of 2016. I did *not* solicit unpublished or "file-drawered" works from contemporary researchers. Since I am interested in historical effects, I did not want to use a method of searching for a study that I could not apply to the entire historical range (i.e., I cannot go back in time and ask researchers from the 1970s for their unpublished works). Third, I attempt to improve on both methodological and theoretical aspects of the review (see Method section).  

The current meta-analysis will test the hypothesis that, due to normative change in recent decades, the ability for explicit measures of interracial attitudes to predict interracial behaviors has decreased; this will be supported if year negatively predicts the effect size of the prediction. I also aim to extend and improve upon previous meta-analyses on ABC in the interracial domain. I account for the dependencies between effect sizes using a robust variance estimation meta-regression technique [@hedges2010robust]. I will also consider if there is a difference in ABC when using affective or cognitive measures of interracial attitudes.  

# Method
## Inclusion Criteria 
I sought out studies that measured both attitudes and behaviors toward a racial outgroup. For the purpose of this meta-analysis, *interracial attitudes* are explicitly expressed feelings, thoughts, behavioral intentions, or other evaluations toward a racial outgroup, a specific racial outgroup member, or policies that pertain to a racial outgroup.  

I did not include implicit measures due to my interest in the expression and suppression of prejudice over time; implicit measures are not as easily fooled by suppression of prejudice as explicit measures, and these measures have been used only in the last two decades. Race-related beliefs that were about the ingroup (e.g., perception of privilege, racial identification), the self (e.g., intrinsic and extrinsic motivation to suppress prejudice), or society (e.g., multicultural or colorblind ideologies, perceptions of norms) were not included; these beliefs do not focus on the racial outgroup. Attitudes that serve as manipulation checks were not included, because they are likely to be more influenced by the experimental manipulation than an individual’s psychological processes.  

*Interracial behaviors* are any actions toward a racial outgroup member or racial outgroup that are—or perceived by the participant to be—consequential [see @talaska2008legitimating]. I determined that the behaviors were inconsequential if the authors described the behavior as “mock,” “fantasy,” “fictitious,” or “hypothetical,” or if participants were instructed to act “as if” or “to imagine” they were in a situation. For situations where consequentiality was unclear, I erred on the side of deciding the behavior was not consequential, and I excluded the study; I reasoned if participants were given a story to make them feel that, for example, their judgments of a job applicant actually affected someone’s chance of getting the job, then the authors would likely include that key information in the manuscript.  

Example behaviors included were: helping an outgroup member, signing a petition for an outgroup, donating money to an outgroup cause, acting aggressively toward an outgroup member, nonverbal behaviors in an interracial interaction (e.g., smiling, seating distance), and the target’s or a confederate’s overall perception of the participant in an interracial interaction.  

Self-reports of past behavior were included under certain circumstances. The vast majority of these instances were participants reporting past interracial contact. Many studies look for the causal link from contact to (a decrease in) prejudice; I did not want to include this type of research, because I am interested in the *opposite* causal relationship. Self-reports were thus only included if they were measured at the same time as the attitude. Longitudinal studies were included, but the effect sizes taken from these studies were between self-reported attitudes and self-reported behaviors at the *same* measurement point.  

I am only interested, as well, in voluntary contact. I only included self-reports that were explicitly voluntary contact: If the measurement tool asked if participants invited Black people into their home, how much leisure time they spent with Black people, etc., then I included the past reported contact. But contact was not included if it was likely involuntary—such as contact at work, school, or in housing (behaviors which are not as easy to control). I also only included quantity of voluntary contact, not quality; quality is not a behavior, but the participant’s subjective assessment of it. Quality of interracial interactions, however, were included if the assessment was from the target or a third party.  

Commitments to future action were included as behaviors if the participant made some form of commitment such that it was likely another person would hold them accountable for it in the future (e.g., providing contact information, signing one’s name). Lastly, I did not include voting behaviors; while voting for one politician or another consequentially affects racial outgroups, these behaviors are likely to be clouded by other (potentially) more relevant factors, such as political identification.  

I established a number of other inclusion criteria that would ensure the collected effect sizes best tested the research question of tracking the correlations over time due to changes in normative acceptability. First, race is socially constructed, and it is sometimes unclear whether or not a given social group is psychologically represented as a “race” in participants’ minds. Groups that are not “races” per se, such as Muslims or people from a different country (e.g., Turks) were included if they likely signaled a racial or ethnic difference to participants. For example, “Muslim” likely communicates “Arab” to White, Western participants; Turkish people are considered a different ethnic group than Germans, whereas Dutch people are not. Second, the target of the interracial attitudes and behaviors must be the same target group; personality studies that included measures of “generalized prejudice” (i.e., measures of prejudice toward a variety of groups) as predictors of behaviors toward one group or individual were not included. Third, the measurement had to be done at the individual level. Studies that reported correlations at the neighborhood, classroom, or other group level were not included, as these do not necessarily capture the intrapersonal psychological processes of suppression and expression. Lastly, I included only studies where the majority of participants were adults, because (a) children are unlikely to have internalized or fully understand the norms surrounding prejudice and (b) all of these studies were self-reported behavior, which is often closely tied to parental attitudes [e.g., @ata2009intergroup].  

## Literature Search
I performed backward and forward searches of narrative and meta-analytic reviews of interracial behaviors and attitude-behavior correspondence [e.g., @richeson2007negotiating; @saucier2005differences; @talaska2008legitimating; @toosi2012dyadic]. I also searched the aggregate database Academic Search Complete and Google Scholar with various combinations of terms, including: prejudice, attitude, stereotype, emotion, racism, behavior, discrimination, etc. These searches included unpublished research, such as theses and dissertations. I read through abstracts and liberally downloaded studies that might meet inclusion criteria. All studies that met criteria upon further reading were then forward and backward searched. I followed this procedure iteratively: If these new searches yielded more studies that met inclusion criteria, these papers in turn were forward and backward searched, and so on. I performed additional specific searches, including papers introducing new measurements of racism—as behaviors are often included as measures if predictive validity—and for dependent variables often used by prejudice researchers (e.g., lost-letter technique, seating distance, helping behavior, interactions).  

## Effect Size and Variance Calculation
Effects were reverse-scored when necessary such that a positive correlation means the more negative an interracial attitude was, the more negative was the interracial behavior. I extracted correlation coefficients from studies where they were directly available. A number of studies median split attitudes, behaviors, or both, or the behaviors were dichotomous choices. If both attitudes and behaviors were dichotomized, I converted the 2 $\times$ 2 contingency tables to tetrachonic correlations—and calculated the variances for these effect sizes—using the `escalc` function in the `metafor` R package [@viechtbauer2010conducting]. In the case of dichotomizing only one of the variables, I converted the (a) means and standard deviations, when they were available, or (b) $t$- and $F$- statistics to biserial correlations. I again used the `escalc` function to calculate both the effect size and variance [@jacobs2016estimation]. One study [@raden1973relationship] reported a one-way ANOVA with three conditions: high, medium, and low prejudice. These means and standard deviations were used to calculate a polyserial correlation and its variance, using the `polyserial` function in the `polycor` R package [@fox2016polycor]. Tetrachoric, biserial, and polyserial correlations were all employed because they assume that the underlying latent variables are continuous. Three studies [@fendrich1967study; @linn1965verbal; @saenger1950customer] provided enough information in the paper (such as detailed frequency tables) to recreate or reasonably approximate the original data, from which correlations were directly calculated. If none of these methods were possible, I e-mailed authors for zero-order correlations between the attitudes and behaviors discussed in their article.  
Lastly, all product-moment, tetrachoric, biserial, and polyserial correlations were converted to $z$ using Fisher's transformation, $arctanh(r)$ [@fisher1921probable]. For product-moment correlations ($k = 217$), the variance was calculated using the standard $1/(N-3)$ formula [@borenstein2009introduction]. The variances for the remaining correlations ($k = 23$) were calculated using the delta method [@severini2005elements, p. 400].  

## Moderators and Coding Procedures 
My primary research question (i.e., has the predictive validity of interracial attitude measures decreased over time?) was tested by using year as a moderator, which is coded in two ways. First, the year a paper was published: I subtracted an article’s publication year minus the minimum date of publication for the entire data set (1950). In the case of dissertations that were later published in peer-reviewed journals, I chose the date of publication. The primary benefit of this method is that it is consistent across all studies. In the results section, I will call this "publication year". The problem, however, is that sometimes there was a large discrepancy between when the data were collected and when they were published. Some—but not all—articles reported date of collection, and some—but not all—articles were earlier defended as dissertations. I also coded the earliest possible date from each publication as another measure of year; I refer to this in the results section as "earliest year." Any relationship between effect size and publication year could be due to how the field has changed measuring the phenomenon, so attitude type and behavior subtlety were included as covariates.  

## Attitude Type
I coded attitudes as (a) cognitive, (b) affective, (c) both, or (d) other or unsure (Table 1). Cognitive measures focused on beliefs or stereotypes about the group. Are they lazy? Would they be better off if only they tried a little harder? This included measures of stereotypes (e.g., rating the outgroup on a number of adjectives, such as lazy or violent) as well as beliefs, such as the Old-Fashioned and Modern Racism Scales [@mcconahay1981has] and the Pro-Black/Anti-Black Attitudes Questionnaire [@katz1988racial].  

```{r table-1, results='asis'}
table1 <- meta %>% 
  group_by(attitude) %>% 
  count() %>% 
  spread(attitude, nn) %>% 
  as.data.frame()
colnames(table1) <- c("Cognitive", "Affective", "Both", "Other or Unsure")
row.names(table1) <- c("$k$")
apa_table(table1, caption="Distribution of Effect Sizes $k$ by Attitude Type", digits=0, placement="h", align="c", row.names=TRUE)
```

Measures were coded as affective if they focused on emotions or overall evaluations of racial outgroups. Does the participant feel positively about the group? Would they be happy to be their neighbor? Do they feel disgusted by them? Overall evaluations (positive, negative) as well as specific emotions (disgust, anger, afraid) were both considered affective measures.  

Many popular measures of prejudice include both cognitive aspects and affective aspects of attitudes, such as Attitudes Toward Blacks [@brigham1993college], Multifactor Racial Attitudes Inventory [@woodmansee1967dimensions], and Subtle and Blatant Prejudice Scales [@pettigrew1995subtle]. If a measure included a significant number of items that measured both emotions and beliefs about groups, I coded them as both.  

Some studies did not provide enough information to discern whether or not they included affective or cognitive elements, while others could not be categorized as affective or cognitive (e.g., behavioral intentions). There were not enough “other” effect sizes to make their own meaningful categories, so all of these left over effect sizes were coded in an “other or unsure” category.  

Attitude type could have been modeled in two ways. The “other and unsure” category does not represent a meaningful theoretical construct. One option is to remove it from any analyses using attitude type as a moderator. This removes any worries that the “other or unsure” category is modeling miscellaneous noise; however, I lose $k$ doing this. Moreover, reporting standards were lower decades ago, so I lose especially precious old $k$ doing this. I performed meta-regression both including and excluding these “other and unsure” effect sizes in attitude type moderator analyses. Analyzing the data in various ways can be informative, provided that it is transparent [@steegen2016increasing], so I report all permutations of year and attitude coding analyses.  

## Behavior Subtlety
The behaviors that researchers tend to study may have gotten subtler over time and thus more difficult to predict from explicit behaviors. Indeed, researchers have focused on showing that explicit measures do not predict these behaviors—instead, implicit measures do. To combat this issue, I modeled and controlled for subtlety of behaviors.  

I grouped all behavioral outcomes into 39 distinct paradigms, with one more being a combination of two of the others (how many interracial friends and how much time spent with them). I wrote descriptions of these 39 paradigms and asked 171 Amazon Mechanical Turk workers to judge the subtlety of the behavioral measures. After telling participants about the basics of the research process, workers read 10 descriptions (randomly selected from the 39) and answered three questions about each on a 0 (*Not at all*) to 100 (*Very much so*) scale: “Would it be easy for the participant in the study to figure out that the behavior they are doing is about race?” “Would it be easy for the participant to fake it so that they look less prejudiced than they actually are?” and “Is this an obvious way to measure negative behaviors towards people of another race?” Each question was answered by at least 40 people. The mean of each of these questions was taken for each different behavioral measure. Correlations between the three measures were higher than .78, so these three questions were averaged and then reverse-scored to create a behavior subtlety score. The Appendix includes descriptions of behaviors, means, and standard deviations of subtlety, as well as directions for participants.  

## Data Analysis
I gathered $k = 240$ effect sizes across $m = 90$ studies. Table 2 describes the distribution across studies: Half of the studies reported more than one effect size, with three studies reporting as many as 15 effect sizes. Traditional meta-analytic methodologies assume that all effect sizes are independent of one another; this assumption is severely violated in the present analysis, as effect sizes drawn from the same participants are dependent on one another.  

```{r table-2, results='asis'}
table2 <- meta %>% 
  group_by(studyid) %>% 
  count() %>% 
  with(table(nn)) %>% 
  as.data.frame() %>% 
  spread(nn, Freq)
colnames(table2) <- as.character(c(1:8,11,12,15))
row.names(table2) <- c("$m$")
apa_table(table2, caption="Number of Studies $m$ with a Given Number of Effect Sizes", digits=0, placement="h", align="c", row.names=TRUE)
```

Many meta-analysts ignore these dependencies. For example, @talaska2008legitimating also find that the majority of their effect sizes are dependent on another effect size; given that their moderator of interest—the type of attitude measure—is at the effect size level, it would have severely limited their $k$ to only choose one effect size per study. The authors thus allow multiple effect sizes per study in their meta-analysis: "We coded effect sizes for every attitude–behavior pair in each study. Although this leads to issues of non-independence in calculating the overall effect size, we were more interested in the effects of moderating variables" (p. 269). Implicit in this statement is that dependencies might bias the overall effect size, but *not* their moderator analyses. Is this the case?  

The first issue with this approach is that studies reporting more effect sizes get more weight; all else being equal, a study with three effect sizes will have three times as much weight in the analysis. The primary issue, however, is that effect sizes based on the same participants have correlated errors, and ignoring these dependencies falsely assumes that the covariance between any two errors is zero. This will underestimate the variances of effect sizes; thus, the standard errors of test statistics in the *moderator* analyses will also be underestimated, leading to an inflation in Type I error [@aarts2014solution; @borenstein2009introduction, Chapter 24; @finch2014multilevel, Chapter 2; @james2013introduction, Chapter 3].  

One would ideally use a multivariate meta-analysis to model these dependencies; however, this requires the meta-analyst to have access to the full covariance matrix of all measures in all studies. This is not realistic in many settings [@jackson2011multivariate], particularly in the present meta-analysis of a literature where (a) researchers hardly publish this information and (b) the research has been published over the course of 70 years, making acquiring this information from the authors of many of theses studies impossible.  

Multilevel meta-analysis has also been proposed as a way to deal with unknown dependencies between effect sizes [@cheung2014modeling; @konstantopoulos2011fixed; @van2013three]. While some argue that individuals could be modeled at Level 1, effect sizes at Level 2, and study at Level 3 [e.g., @cheung2014modeling], three-level meta-analyses still assume that residual errors are orthogonal within clusters [@tanner2016handling]. This assumption is violated when multiple effect sizes are drawn from the same participants.  

The most recent development [@polanin2017review] for modeling these dependencies and avoid underestimating standard errors is to use robust variance estimates [RVE; @hedges2010robust; @tanner2016handling]. I performed my meta-analysis using RVE for correlated effects in the `robumeta` R package [@fisher2015robumeta].  

As mentioned above, I am able to calculate the variances of effect sizes directly from sample size, but I am *not* able to calculate the covariance between effect sizes. RVE solves this problem by using the cross products of the residuals for each study to estimate the variance-covariance matrix of effect sizes within a study. While the estimate of the covariance matrix in each study is not very good, the combined variance estimate converges to the true variance as the number of studies approaches infinity [@hedges2010robust].  

Traditional meta-analyses weight effect sizes by using the inverse of the variance. RVE weights each effect size using (a) the inverse of the average variance across all effect sizes in a study (assuming a constant correlation across effect sizes) (b) divided by the number of effect sizes in the study. This ensures that a study does not get "extra" weight simply by having more effect sizes.  

This method is used primarily for the purposes of this meta-analysis: interpreting meta-regression coefficients. The variance estimates found in other meta-analyses (e.g., $Q, I^2, \tau^2$) are not precise when using RVE. Given this shortcoming of RVE—and my main focus in interpreting meta-regression coefficients—I will not focus on these estimates. The Appendix includes data, code, and detailed analyses.  

```{r figure-1, fig.cap="Distribution of effect sizes. Total $k$ = 240. The vertical line represents meta-analytic effect size."}
ggplot(meta, aes(r))+
  geom_vline(xintercept=0.21, linetype=2)+
  geom_histogram(binwidth=.03, alpha=.8)+
  theme_light()+
  labs(x="Effect Size", y="Count") +
  annotate("text", x=.33, y=17, label="r = .21")
```

# Results
I first ran an intercept-only model first to get the overall meta-analytic effect, $r = .21$ $[.17, .25]$. Figure 1 displays the distribution of these effect sizes, marking the meta-analytic average with the dotted vertical line. This summary effect size is equal to the smallest effect size found in previous ABC meta-analyses [@notani1998moderators]. This estimate should be among the lowest, given that the sensitive subject matter of race may lead participants to respond in a socially desirable way, obscuring counter-normative attitudes.  

## Year
I predicted the effect size from publication year and earliest year in two separate meta-regression equations. In both cases, the results were the same: $b = -.002$ $[-.005, 000]$, $t(26) = -1.91$, $p = .067$. This is thin evidence that the relationship between interracial attitudes and behaviors decreases over time at a rate of $z = .002$ per year. This is not a standardized regression coefficient; although it may seem small, the predicted $r$ for the first study (i.e., the intercept) was $.32$, and the model-predicted value in 2016 (the most recent publication) is about $.17$. The $p$-value for these analyses warrant skepticism, but there is some evidence that interracial ABC decreases over time. But is this due to the theoretical construct of interest—normative climate—or other factors?  

## Full Model
I regressed effect size on (a) year, (b) behavior subtlety, (c) attitude type, and (d) the methodological covariate of whether or not the effect size was transformed. Tables 3 and 4 contain four separate meta-regression models, including all combinations of year and attitude type. In each model, effect size does not depended on year, $ps > .802$. Effect size did not depend on attitude type, either: all omnibus test $ps > .313$. The only significant predictor was subtlety, $ps < .009$, which negatively predicted effect size. Note that `robumeta` implements RVE using an adjustment to the degrees of freedom to account for small sample sizes (such as those found in meta-analyses), which causes the degrees of freedom to differ from coefficient to coefficient [@fisher2015robumeta; @tipton2015small].  

```{r table-3, results='asis'}
mod3.rve <- robu(formula=es ~ pubyear + subtle + attitude + transformed, 
                 data=meta, studynum=studyid, var.eff.size=var, 
                 modelweights="CORR")

mod4.rve <- robu(formula=es ~ earlyyear + subtle + attitude + transformed, 
                 data=meta, studynum=studyid, var.eff.size=var, 
                 modelweights="CORR")

table3a <- as.data.frame(mod3.rve$reg_table[1:8])
table3b <- as.data.frame(mod4.rve$reg_table[1:8])
table3a$labels <- c("Intercept", "Publication Year", "Subtlety", "Attitude (Affective)", "Attitude (Both)", "Attitude (Other)", "Transformed")
table3b$labels <- c("Intercept", "Earliest Year", "Subtlety", "Attitude (Affective)", "Attitude (Both)", "Attitude (Other)", "Transformed")
table3 <- bind_rows(table3a, table3b)
colnames(table3) <- c("Predictor", "$b$", "$SE$", "$t$", "$df$", "$p$", "CI UB", "CI LB")
apa_table(table3, caption="Meta-Regression Model, Using All Data", midrules=7, placement="h", align="c", digits=3,
          note="CI = Confidence Interval, LB = Lower Bound, UB = Upper Bound.
                All confidence intervals reported are 95 percent.")
```

The thin evidence for interracial ABC diminishing over time provided by regressing effect size on year alone was nonexistent after controlling for relevant covariates. The only reliable predictor was subtlety: The more subtle a behavior, the less predictive an explicit interracial attitude. This analysis also fails to replicate previous work suggesting emotional measures are better predictors of discrimination than are cognitive ones; the ABC was about $r = .035$ higher for affective than cognitive attitude measures, but not significant $ps > .40$.  

```{r table-4, results='asis'}
mod5.rve <- robu(formula=es ~ pubyear + subtle + attitude + transformed, 
                 data=meta_noatt3, studynum=studyid, var.eff.size=var, 
                 modelweights="CORR")

mod6.rve <- robu(formula=es ~ earlyyear + subtle + attitude + transformed, 
                 data=meta_noatt3, studynum=studyid, var.eff.size=var, 
                 modelweights="CORR")

table4a <- as.data.frame(mod5.rve$reg_table[1:8])
table4b <- as.data.frame(mod6.rve$reg_table[1:8])
table4a$labels <- c("Intercept", "Publication Year", "Subtlety", "Attitude (Affective)", "Attitude (Both)", "Transformed")
table4b$labels <- c("Intercept", "Earliest Year", "Subtlety", "Attitude (Affective)", "Attitude (Both)", "Transformed")
table4 <- bind_rows(table4a, table4b)
colnames(table4) <- c("Predictor", "$b$", "$SE$", "$t$", "$df$", "$p$", "CI UB", "CI LB")
apa_table(table4, caption="Meta-Regression Model, Excluding \"Other\" Attitudes", midrules=6, placement="h", align="c", digits=3)
```

## Sensitivity Analysis
The meta-analyst inputs a value $\rho$ as an estimate of the within-study effect size correlation needed to initially calculate $\tau^2$ in the RVE approach to meta-regression with correlated effects. I used the package's default $\rho = 0.80$ for all models in this paper. I conducted sensitivity analyses for each model to test if coefficients and standard errors depended on this $\rho$; changing the value of $\rho$ from 0 to 1 altered neither the coefficients nor the standard errors. This means that the results reported here are *not* dependent on a subjective decision needed to estimate RVE.  

## Ancillary Analyses
I also ran a model regressing effect size on subtlety alone; as behaviors were more subtle, the effect size was smaller, $b = -0.006,$ $SE = 0.002,$ $t(35.9) = -3.39,$ $p = .002$. I also regressed effect size on attitude type alone. When *including* the "other or unsure" effect sizes in the data set, the omnibus test for attitude type was not significant, $F(32.3) = 2.52$, $p = .076$. There was also no relationship when *excluding* this miscellaneous category from the data set, $F(32) = 1.21$, $p = .313$; the marginal effect in the model including "other or unsure" effect sizes was driven by that category.  

## Research Trends
These data also allow an examination of research trends over time. Have researchers turned to more subtle behavioral measures? To answer this question, I ran two separate multilevel models, one using publication year and another using earliest year to predict behavior subtlety. Subtlety was measured at Level 1 (effect size) and nested within Level 2 (study), where year was the predictor. A random intercept-only model was compared to models with just publication year or earliest year as predictors, using likelihood ratio tests [@hox2010multilevel]. There was thin evidence for both publication year predicting greater subtlety, $b = 0.099,$ $\chi^2(1) = 2.73,$ $p = .099$, as well as earliest year, $b = 0.113,$ $\chi^2(1) = 3.57,$ $p = .059$.  

But many of the effect sizes were not *observed* behaviors, but self-reports of past behaviors. Measurements for these types of behaviors have remained consistent over time (e.g., simply asking people how much they spend time with someone of the target race). Out of the 240 effect sizes collected, 149 (62%) were observed. Running the same analyses of year predicting subtlety on just these 149 data points, both publication year, $b = 0.13,$ $\chi^2(1) = 5.50,$ $p = .019$, and earliest year, $b = 0.126,$ $\chi^2(1) = 5.32,$ $p = .021$, positively predict behavioral subtlety. When excluding self-reported behaviors, researchers are choosing to observe subtler discriminatory behaviors in their experiments now than they did in the past.  

I also examined what types of attitude measures researchers have chosen over time. I fit three separate logistic multilevel models predicting each type of attitude from year [@begg1984calculation], using the data without the "other or unsure" effect sizes. Each model had a dichotomous outcome: First, cognitive vs. affective or both; second, affective vs. cognitive or both; third, both vs. cognitive or affective. As with subtlety over time, attitude was a Level 1 (effect size) dependent variable nested within Level 2 (study), where year was the predictor. Neither publication year nor earliest year were significant predictors of any of the three categorical dependent variables, all $\chi^2(1) < 0.76,$ $ps > 0.385$; there is no relationship between year and researcher's choice of attitude measure type.  

## Diagnostics
The overall meta-analytic effect size between interracial attitudes and behavior is $r = .21$. Despite thoroughly searching for articles, dissertations, and theses to include in this meta-analysis, I certainly missed studies; additionally, publication bias exists and may undermine this meta-analytic result. Meta-analytic tools that handle correlated effect sizes are relatively new, and testing for publication biases are rarely mentioned in the literature [e.g., @cheung2014modeling; @fisher2015robumeta; @jackson2011multivariate; @tanner2016handling]. Nonetheless, I test and examine the possible influences of bias by calculating fail-safe $N$, creating a funnel plot, and conducting a $p$-curve analysis.  

I conducted the fail-safe $N$ [@orwin1983fail] to estimate the number of studies with an effect size of zero that I would have to observe for the meta-analytic $r$ to become trivial. Along with other researchers, I consider $r = .10$ to be a trivial effect size here [@ferguson2013eye; @hyde2006gender; @lambert2012impact; @murphy1999testing; @uttal2013malleability]. I would need to observe 105 studies with $r = .00$ for the the correlation to become trivial.  

Funnel plots may be asymmetric for a number of reasons beside publication bias, including effect sizes being dependent on one another [@nakagawa2017meta]. For this reason, I generated a funnel plot based on a standard random-effects, intercept-only meta-analysis where effect sizes and variances are averaged together at the study level, an approach used by other researchers using an RVE meta-analytic model [@uttal2013malleability]. The model was fit using the `rma` function and in the `metafor` R package [@viechtbauer2010conducting]. This funnel plot (Figure 2) is mostly symmetric, save for two outliers.  
  
```{r figure-2, fig.cap="Funnel plot of effect sizes (effect sizes and variances averaged by study)."}
funneldata <- meta %>%
  group_by(studyid) %>% 
  summarise(m_es=mean(es), m_var=mean(var))

funnelmodel <- rma(m_es, m_var, data=funneldata)
funnel(funnelmodel)
```

Lastly, I conducted a $p$-curve analysis to examine evidence of publication bias or $p$-hacking [@simonsohn2014pps; @simonsohn2014jepg; @simonsohn2015better]. When the null hypothesis is true, $p$-values testing that null hypothesis are uniformly distributed; when the alternative hypothesis is true, $p$-values will be positively skewed, and smaller $p$-values are more likely to be observed. The $p$-curve takes advantage of these probabilities by plotting the percentage of findings at each successive significance level: $p = .05$, $p = .04$, etc. When results are $p$-hacked or lack statistical power, the researcher examines more $p$-values around the $p = .05$ threshold than would be expected. When results contain "evidential value," the curve is appropriately positively-skewed, with the largest percentage of $p$-values congregating at the $p < .01$ level.  
  
```{r figure-3, fig.cap="$P$-curve of statistically significant results (generated via the $p$-curve app).", out.width="90%"}
include_graphics("/Users/markiiwhite/Dropbox/CompsMeta/Manuscripts/pcurve.png")
```

Figure 3 displays the $p$-curve. Notably, 115 of the 240 tests were not statistically significant; this is to be expected—even in the presence of a true effect—when a field generally conducts under-powered tests [@lakens2017too]. It also supports the observation @kraus1995attitudes made in his meta-analysis that non-significant ABC relationships are more likely to be published than other null results, given their counter-intuitive nature. Of the 125 statistically significant tests, 107 (86%) were $p < .025$. Using a binomial test, this is significantly different from the 50% that would be expected from the uniform distribution if there were no effect, $p < .0001$. I conducted this test using Version 4.052 of the $p$-curve app ([www.p-curve.com](www.p-curve.com)); recently, the authors have implemented other tests—such as a continuous test instead of a binomial one—to test for evidentiary value [@simonsohn2015better]. Each of these tests demonstrates evidentiary value, $p < .0001$.  

# Discussion
There is little evidence that the correspondence between interracial attitudes and interracial behaviors in the social psychological literature has decreased over time because of normative changes in the expression of prejudice. Year, when considered alone, tenuously predicted smaller effect sizes. However, relevant covariates fully accounted for this relationship: When considering year, behavior subtlety, attitude type, and whether or not the effect size was transformed together, only the subtlety of the behavior predicted the size of the effect. The more subtle the observed behavioral measure, the smaller the correlation between explicit interracial attitude and behavior.  

Many researchers begin papers by noting the paradox that self-reports of prejudice have decreased in recent decades, yet discrimination continues to persist. If this is true, then psychologists should have gotten worse at predicting discrimination from self-reports over time: Attitude-behavior correspondence (ABC) in the interracial domain should correlate negatively with the year in which the study was published. With this hypothesis, year is treated as a proxy for stricter norms against prejudice. There was debatable evidence for this negative relationship when considering the zero-order relationship, but there was no relationship after considering covariates. There may be indirect evidence for the hypothesis, however. Psychological measurement and methods have improved over time, and meta-analyses of ABC across content domains have found a *positive* relationship with year [@glasman2006forming; @kraus1995attitudes]. ABC in the interracial domain not following the improved correspondence in other domains could be construed as indirect evidence that egalitarian norms have kept the correlation between prejudice and discrimination from similarly increasing. However, researchers have been influenced by the same changing normative climate; no longer do we ask participants when they think lynching is justified—we ask participants how much they agree with more coded prejudiced statements. Researchers could have preempted finding the hypothesized correlation by switching to better measurements of prejudice as the old measurements exhibited poor psychometric (e.g., floor effect) and predictive properties.  

Non-self-report measures of prejudiced behavior have gotten more subtle over time, a trend likely influenced by the focus on implicit prejudice. Various studies show that explicit (i.e., self-reported) prejudice predicts controlled, purposeful behavior while implicit prejudice predicts subtle, automatic behaviors [@pearson2009nature]. I observed meta-analytic evidence for half of this contention: The subtler the behavior, the worse the predictive power explicit measures have. Therefore, research has changed in two ways that mask the ability to observe the relationship between year (as a measurement of norms) and ABC: first, researchers are using measures that keep up with changing normative trends; second, researchers turned their interest toward the newer, implicit theories of prejudice, measuring subtler behaviors that implicit measures are said to predict. Given the evidence of controlled, purposeful prejudiced behaviors in the recent political climate [@cuevas2016trump; @holley2017texas; @lombroso2017hail; @stolberg2017man; @thrush2017trump; @wang2017final], psychologists might consider a refocus on theories of explicit prejudice and behavior.  

## Small Effects, and A Focus on Prediction
Many researchers have lamented how social and personality psychology lacks a focus on measuring *actual* behaviors, calling for more of a focus on them, with a particular interest in *predicting* behaviors [e.g., @baumeister2007psychology; @cialdini2009we; @furr2009personality]. In the area of stereotyping, prejudice, and discrimination specifically, @fiske2000stereotyping argued that social and personality psychologists need "to get serious about predicting behavior... The alarm is urgent" (p. 312). How well can we predict interracial behaviors from interracial attitudes?  

These data yield a meta-analytic correlation of $r = .21$, a smaller estimate than found by both @talaska2008legitimating, $r = .26$, and @schutz1996strong, $r = .29$. A first reaction to this estimate might be to square it and bemoan how we can only account for about 4% of the variance in interracial behaviors with interracial attitudes. Researchers have felt this sense of disappointment in attitudes ever since @lapiere1934attitudes found that, while most establishments said over the phone that they would *not* permit Chinese people to eat in their restaurant or stay in their hotel, only *one* of these establishments actually denied service to a Chinese couple.  

Researchers have debated how to interpret the size of ABC effects for decades [see @ajzen2005influence]. In response to the earliest studies showing a lack of consistency between attitudes and behaviors, @defleur1963attitude called it a "fallacy" to expect strong ABC (p. 27). @wicker1969attitudes similarly remarked that "it is considerably more likely that attitudes will be unrelated or only slightly related to overt behaviors than that attitudes will be closely related to actions" (p. 65); recent meta-analysts came to similar conclusions that interracial attitudes do not predict behaviors as much as we would like them to [@oswald2013predicting; @schutz1996strong].  

Others are not as pessimistic: @kraus1995attitudes argues that his observed $R^2 = .14$ is not really that small and that $R^2$ is often misinterpreted; @ajzen2005influence argue correlations will be higher if researchers choose "representative" behavioral measures (or average across many behaviors). Many have made suggestions on how to improve ABC [e.g., @oswald2013predicting suggests using multiple, heterogeneous attitude measures in latent variable models to account for measurement error], but I wish to offer two related suggestions on increasing our ability to *predict* discrimination: first, adapting statistical methods aimed at *prediction* more than at *inference*; and second, relaxing the focus on just interracial attitudes.  

@james2013introduction present two motivations for conducting statistical analyses: *inference* and *prediction.* The goal of inference is to understand the relationship between variables. This is the approach social and personality psychologists take: We generally perform null hypothesis significance tests to tell us whether or not some relationship between variables is present, and we make theoretical inferences from these data. We know that there is a positive relationship between interracial attitudes and behavior. What we should move toward now is prediction, where the goal is to make accurate guesses at some outcome, given a collection of input variables. This is not what social and personality psychologists do. However, as @fiske2000stereotyping noted, we should be moving toward using our various theories to *predict* discrimination.  

Lewin's famous equation says that behavior is a function of a person and their environment, $B = f(P, E)$ [@lewin1951field]. When prejudiced behavior is our dependent variable, our job is to find some function $f$ that explains $B$. Personality and social psychologists rely on general linear models as our estimation of $f$, which serves the purpose of inference well. These models, such as ordinary least squares (OLS) regression, are easy to interpret. However, these models make rigid assumptions: In the present case, we assume that the relationship between attitudes and behavior is strictly linear.  

If we are to move toward a focus on *prediction*, we should embrace the vast world of algorithms beyond OLS. Our foremost concern for prediction is to minimize the difference between what our models predict and what we actually observe. Unless the relationship is strictly additive and linear, we can often accomplish this better by using, for example, non-parametric models (e.g., regression and classification trees) that do not assume a rigid form for $f$ [see @james2013introduction for an overview]. However, this comes with the trade-off that the models are not always as interpretable.  

But shiny, cutting edge algorithms to estimate $f$ are not a magic solution. If we wish to estimate behavioral prejudice $B$, we should remember Lewin's equation that it is a function of both the person *and their environment.* If we assume $B = f(P, E)$ is correct, it should be no surprise that we observe modest $R^2$ values when we consider only *one* aspect of the wide variety of factors that make up $P$. The field of social and personality psychology has been tremendously successful as a field at generating many useful theories of prejudice, discrimination, and stereotyping. Each of these theories contribute key insights into which aspects of $P$ and $E$ we should consider when trying to predict discrimination. Instead of competition between theories, prediction requires using important variables from *many theories* as inputs in our model. Predicting prejudiced behaviors accurately will involve a large number of these inputs [e.g., social norms, @crandall2005conformity; moral credentials, @monin2001moral; motivation, @plant1998internal; fatigue, @govorun2006ego] and, of course, a correspondingly larger $n$ on which to train and test models.  

Programmers without the crucial domain expertise in prejudice, discrimination, and stereotyping have produced racist and otherwise biased models [@angwin2016machine; @oneil2016weapons]. One lesson is to enlist those with domain expertise, such as social and personality psychologists, when creating these models. Another lesson is to take Fiske's suggestion seriously: If predicting discrimination is our goal, we should be leveraging our theories to create models (a) with *many* predictors across *many* theories and (b) that are less rigid than traditional regression and analysis of variance.  

There is great potential here. For example, consider the data scientists who are predicting and preventing adverse police incidents, using various inputs—from past allegations to sick days taken—and various methods (especially tree-based algorithms). When compared to the existing systems, the model trained by those working with the Data Science for Social Good organization "would have correctly flagged 10-20% more officers who were later involved in adverse incidents and reduced the number of incorrect flags by 50% or more," while a model trained for another police department "was able to correctly flag 80% of officers who would go on to have an adverse interaction, while only requiring intervention on 30% of officers to do so" [@carton2016identifying; @mitchum2017police]. A linear approximation using one predictor—interracial attitudes—is simply not enough to accurately predict something as complex as discrimination; we should not be surprised by the modest meta-analytic relationship found here and elsewhere.  

## Attitude Type
I did not find evidence that affective measures predicted behavior better than cognitive ones; however, @talaska2008legitimating subtitle their article "Emotions, Not Beliefs, Best Predict Discrimination" from their meta-analysis of the same phenomenon and many of the same studies. Why does their conclusion differ from the present meta-analysis?  

First, the authors treated *aggregated* measures of specific emotions (e.g., fear, disgust) to be qualitatively different than general affective measures, a differentiation that is not supported by theories of specific prejudices [@cottrell2005different]. For example, Talaska and colleagues coded measures as "emotion" if they went "beyond overall positive/negative valence" (p. 268). There were two other affective categories: "Overall valence" measures were "*overall* positive versus negative evaluation, without reference to specific beliefs, stereotypes, emotions, or behaviors," while "valence and emotion" measures included "feelings limited to simple positive-negative connotations" (p. 268). @talaska2008legitimating do not differentiate between differentiated emotions, however. All measures of specific emotions are aggregated into an "emotion" category; proponents of specific prejudiced emotions argue *against* aggregating across multiple types of emotions, because it masks the very specifics that these theories try to explain [see @cottrell2005different].  

I did not distinguish between differentiated emotions and overall valence, because there are not enough studies using specific emotions to code for each emotion (e.g., contempt, fear, anger, disgust) separately. This was also done for theoretical reasons. @talaska2008legitimating cite the primacy of affect—that people often evaluate attitude objects, with little effort, and with no thought required—to argue why emotions should be the primary motivators for behavior. @zajonc1980feeling did argue that feelings would be better predictors of behaviors: "Quite often 'I decided in favor of X' is no more than 'I liked X.' Most of the time, information collected about alternatives serves us less for making a decision than for justifying it afterward" (p. 155). However, the theory is about affect, not specific emotions: @zajonc1980feeling refers to "general" feelings that are about "approach-avoidance", while "other emotions, such as surprise, anger, guilt, or shame... are ignored" (p. 152). Aggregated measures of specific emotions and overall evaluations both reflect overall affect, given that these measures average across specific emotions that fall on a general negative-to-positive continuum [@lindquist2013emotions; @russell2003core].  

Second, they do not report any pairwise comparisons between overall evaluations and any other attitude type, so it is unclear how general affective measures performed in their meta-analysis. There were nine attitudinal categories in total. Talaska and colleagues collapsed across all stereotype and belief categories, reducing the total number to seven categories. However, the comparisons between attitude types (shown in Figure 2, p. 282) only display five of these categories. The "emotion and overall valence" category and "stereotype and overall valence" category (e.g., identifying how good vs. bad are outgroups members) are missing. It is unclear if these two measures were included with the "mixed/other" category [e.g., IMS/EMS, @plant1998internal]. Yet, the median effect sizes for differentiated emotions ($r = .35$) and emotion and overall valence ($r = .32$) measures were of about the same strength (Table 7, p. 281).  

An omnibus test for type of attitude was reported to be significant, $p < .001$, but only one follow-up test was reported: emotion-focused attitudes had a larger effect size than stereotypes and/or beliefs attitudes, $p < .001$. The direct comparison between differentiated emotions and overall evaluations was not tested, which is the strongest test for the importance of specific emotions. It is unclear if this test was done, why only five of the seven categories were included in the omnibus test, and if the necessary alpha-level corrections were done if multiple comparisons were made.  

Third, the majority of the effect sizes in Talaska and colleagues' meta-analysis are dependent on at least one other effect size, as many effects are gathered from the same participants. I correct for the underestimation in standard errors that this causes, while Talaska and colleagues do not.  

Is their argument that emotions are better predictors of discrimination than beliefs wrong? Not necessarily. Their meta-analysis does not test the predictions derived from theories of specific prejudices and differentiated emotions. To test these hypotheses, one must be able to differentiate between the specific emotions in analysis. There is not enough literature examining the link between *specific* prejudices (e.g., fear, disgust) and correspondingly *specific* behaviors yet. Researchers interested in these theories should examine actual behaviors, not just behavioral tendencies or intentions. For example, *anger* might outperform a feeling thermometer when it comes to predicting *aggressive* behaviors specifically; *pity* might be the best predictor of benevolently prejudiced behaviors. These are interesting empirical questions that need more study before they be examined meta-analytically.  

Is *affect* no more important than beliefs in predicting discrimination? This analysis provides evidence that it is not. Yet, it is unclear if affective self-reports are bona fide measurements of affect. "Prejudice" is a latent construct—we do not measure directly and resort to measure indicators of it. Affective self-reports are *expressions* of prejudice, which are often quite different from genuine prejudice itself [@crandall2003justification]. Affective self-reports likely measure underlying affect itself, but also: willingness to admit that one is prejudiced, how justified one thinks the prejudice is, perceived normative support, among many other constructs.  

@zajonc1980feeling, in his argument that affect is primary, reviews evidence that affect is hard to verbalize; even if people *wanted* to be truly honest on affective self-reports, it would be hard for them to do so. He notes that affect may be more tied up in physiology than it is in communication, and modern constructionist theories of emotion have found evidence that supports the idea that overall effect has its foundations in biological reactions, which are then interpreted and communicated to others [@barrett2017theory; @lindquist2015brain; @satpute2015involvement]. More research correlating physiological and neurological measures with interracial discrimination should be done to see if these measures (a) represent negative affect felt toward other groups (i.e., prejudice) better than self-report measures and (b) are particularly strong predictors of behavior. Despite these exciting new ways of measuring affect, I share the sentiment of @crandall2003justification, who are "pessimistic about psychologists’ ability to directly measure the unfiltered psychological reality of prejudice" (p. 437).  

## Conclusion
@omi1994racial note that matters of race in the United States are "maddeningly complex." The current meta-analysis aimed to support the idea that self-reports of prejudice are decreasing at a quicker rate than prejudiced behaviors. In line with the muddy findings across various disciplines, I found no clear evidence to support my hypothesis, largely due to researchers adapting to the same norms I set out to capture. Contrary to previous findings, I did not find evidence that affective measures are better predictors of behavior than cognitive ones. I also hope that psychologists might consider broadening their focus if their goal is to predict discrimination.  

\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
